{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Ray\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">\n",
    "\n",
    "## About this notebook\n",
    "\n",
    "### Is it right for you?\n",
    "\n",
    "This is an introductory notebook that offers a broad overview of the Ray project. The ideal learner will find themselves in the following scenarios:\n",
    "\n",
    "* You are new to Ray and are looking for a project primer.\n",
    "* You are interested in how Ray, a Python first distributed computing framework, can scale your Python applications and machine learning workloads.\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "For this notebook, you should satisfy the following requirements:\n",
    "\n",
    "* Practical Python and machine learning experience\n",
    "* No prior experience with Ray or distributed computing\n",
    "\n",
    "### Learning objectives\n",
    "\n",
    "* Understand what Ray is.\n",
    "* Recognize key characteristics of Ray.\n",
    "* Tour the three layers of Ray.\n",
    "    * Ray Core\n",
    "    * Native libraries\n",
    "    * Ecosystem of integrations\n",
    "* Explore the most common Ray use cases.\n",
    "* Implement a regression task.\n",
    "    * Sequentially in generic Python\n",
    "    * In parallel with Ray\n",
    "* Identify where to go next with Ray.\n",
    "\n",
    "### What will you do?\n",
    "\n",
    "In *Part 1* of this notebook you will learn about Ray project. Then, in *Part 2* you will run an illustrative code example that will give you better \"feel\" for Ray."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Ray project\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/ray_project.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray is one of the leading open source ML projects. (date accessed: Nov 2, 2022)|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "#### What is Ray?\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://www.ray.io/\" target=\"_blank\">Ray</a></strong> is an open-source unified compute framework that makes it easy to scale AI and Python workloads.\n",
    "</div>\n",
    "\n",
    "Ray provides the compute layer to scale applications without becoming a distributed systems expert. These are some key processes that Ray automatically handles:\n",
    "\n",
    "* **Orchestration.** Managing the various components of a distributed system.\n",
    "* **Scheduling.** Coordinating when and where tasks are executed.\n",
    "* **Fault tolerance.** Ensuring tasks complete regardless of inevitable points of failure.\n",
    "* **Auto-scaling.** Adjusting the number of resources allocated to dynamic demand.\n",
    "\n",
    "To lower the effort needed to scale compute intensive workloads, Ray takes a Python-first approach and integrates with many common data science tools. This allows ML practitioners to parallelize Python applications from a laptop to a cluster with minimal code changes.\n",
    "\n",
    "#### Distributed computing: a bit of context and project history\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/ai_compute_annotated.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Amount of compute used in the largest AI training runs far outpaces the processing power of individual CPUs, GPUs, and TPUs. Original diagram from [OpenAI](https://openai.com/blog/ai-and-compute/) with overlaid annotations.|\n",
    "\n",
    "Distributed computing is becoming increasingly relevant for modern machine learning systems. OpenAI's recent paper [AI and Compute](https://openai.com/blog/ai-and-compute/) suggests that the amount of compute needed to train AI models has roughly doubled every 3.5 months since 2012.\n",
    "\n",
    "However, distributed systems are hard to program. Scaling a Python application to a cluster introduces challenges in communication, scheduling, security, failure handling, heterogeneity, transparency, and much more.\n",
    "\n",
    "This context drove the development of Ray: a solution to enable developers to run Python code on clusters without having to think about how to orchestrate and utilize individual machines. The same researchers who created Ray at the University of California Berkeley's [RISELab](https://rise.cs.berkeley.edu/) (the successor to the [AMPLab](https://amplab.cs.berkeley.edu/about/) that created [Apache Spark](https://spark.apache.org/) and [Mesos](https://mesos.apache.org/)) founded [Anyscale](https://www.anyscale.com/)——a managed Ray platform that offers hosted solutions for Ray applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Ray characteristics\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/python_first.jpeg\" width=\"70%\" loading=\"lazy\">|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/simple_and_flexible_api.jpeg\" width=\"70%\" loading=\"lazy\">|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/scalability.jpeg\" width=\"70%\" loading=\"lazy\">|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/heterogeneous_hardware.jpeg\" width=\"70%\" loading=\"lazy\">|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "|Python first approach|Simple and flexible API|Scalability|Support for heterogeneous hardware|\n",
    "\n",
    "#### Python first approach\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/python_first.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "Ray allows you to flexibly compose distributed applications with easy to use primitives in native Python code. This way, you can scale your existing workloads with minimal code changes. Getting started with Ray Core involves just a few key abstractions:\n",
    "\n",
    "1. [**Tasks**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks). Remote, stateless Python functions\n",
    "1. [**Actors**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors). Remote, stateful Python classes\n",
    "1. [**Objects**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects). Tasks and actors create and compute on objects that can be stored and accessed anywhere in the cluster; cached in Ray's distributed [shared-memory](https://en.wikipedia.org/wiki/Shared_memory) object store\n",
    "\n",
    "You will learn more about these abstractions in the [Ray Core tutorials](https://github.com/ray-project/ray-educational-materials/tree/main/Ray_Core).\n",
    "\n",
    "#### Simple and flexible API\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/simple_and_flexible_api.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "##### Ray Core\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/walkthrough.html\" target=\"_blank\">Ray Core</a></strong> is an open-source, Python, general purpose, distributed computing library that enables ML engineers and Python developers to scale Python applications and accelerate machine learning workloads.\n",
    "</div>\n",
    "\n",
    "Acting as the foundational library for the whole ecosystem, Ray Core provides a minimalist API that enables distributed computing. With just a few methods, you can start building distributed apps:\n",
    "\n",
    "* **`ray.init()`**  \n",
    "Start Ray runtime and connect to the Ray cluster.\n",
    "* **`@ray.remote`**  \n",
    "Decorator that specifies a Python function or class to be executed as a task (remote function) or actor (remote class) in a different process.\n",
    "* **`.remote`**  \n",
    "Postfix to the remote functions and classes; remote operations are *asynchronous*.\n",
    "* **`ray.put()`**  \n",
    "Put an object in the in-memory object store; returns an object reference used to pass the object to any remote function or method call.\n",
    "* **`ray.get()`**  \n",
    "Get a remote object(s) from the object store by specifying the object reference(s).\n",
    "\n",
    "*(In the second part of this notebook you will see illustrative example for some of these methods.)*\n",
    "\n",
    "##### Ray AI Runtime (AIR)\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python, domain-specific set of libraries that equips ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Ray AI Runtime (AIR) (sometimes referred to as native libraries and ecosystem of integrations) provides higher level APIs that cater to more domain-specific use cases. Ray AIR enables data scientists and ML engineers to scale individual workloads, end-to-end workflows, and popular ecosystem frameworks, all in Python.\n",
    "\n",
    "#### Scalability\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/scalability.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "Ray allows users to utilize large compute clusters in an easy, productive, and resource-efficient way.\n",
    "\n",
    "Fundamentally, Ray treats the entire cluster as a single, unified pool of resources and takes care of optimally mapping compute workloads to the pool. By doing so, Ray largely eliminates non-scalable factors in the system. \n",
    "\n",
    "Some examples of successful user stories include the following:\n",
    "* [Instacart](https://www.youtube.com/watch?v=3t26ucTy0Rs) uses Ray to power their large scale fulfillment ML pipline.\n",
    "* [OpenAI](https://twitter.com/anyscalecompute/status/1562136159135973380) trains their largest models (including ChatGPT).\n",
    "* Companies like [HuggingFace and Cohere](https://www.youtube.com/watch?v=For8yLkZP5w) use Ray Train for scaling model training.\n",
    "\n",
    "A notable strength is Ray's [autoscaler](https://docs.ray.io/en/latest/cluster/key-concepts.html#autoscaling) implements automatic scaling of Ray clusters based on the resource demands of an application. The autoscaler will increase worker nodes when the Ray workload exceeds the cluster's capacity. Whenever worker nodes sit idle, the autoscaler will scale them down.\n",
    "\n",
    "#### Support for heterogeneous hardware\n",
    "\n",
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/heterogeneous_hardware.jpeg\" width=\"100px\" loading=\"lazy\">\n",
    "\n",
    "Heterogeneous systems present new challenges to distribution because each compute unit has its own programming model. Ray natively supports heterogeneous hardware to achieve load balancing, coherency, and consistency under the hood. All you need to do is specify hardware when initializing a task or actor. For example, a developer can specify in the same application that a one task needs 128 CPUs, another task only requires 0.5 GPUs, and an actor requires 36 CPUs and 8 GPUs.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/heterogeneous_hardware_code.png\" width=\"60%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Easily specify amount of resources needed, by using `num_cpus` and `num_gpus`|\n",
    "\n",
    "An illustrative example is the production [deep learning pipeline at Uber](https://www.anyscale.com/ray-summit-2022/agenda/sessions/215). A heterogeneous setup of 8 GPU nodes and 9 CPU nodes improves the pipeline throughput by 50%, while substantially saving capital cost, compared with the legacy setup of 16 GPU nodes.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/uber.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Deep learning pipeline at Uber using a heterogeneous hardware setup with Ray.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ray libraries\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/map.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stack of Ray libraries - unified toolkit for ML workloads.|\n",
    "\n",
    "There are four layers to Ray's unified compute framework:\n",
    "\n",
    "1. Ray cluster\n",
    "1. Ray Core\n",
    "1. Ray AI Runtime (native libraries)\n",
    "1. Integrations and ecosystem\n",
    "\n",
    "#### Ray cluster\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/cluster/getting-started.html\" target=\"_blank\">Ray cluster</a></strong> is a set of worker nodes connected to a common Ray head node. Ray clusters can be fixed-size, or they can autoscale up and down according to the resources requested by applications running on the cluster.\n",
    "</div>\n",
    "\n",
    "Starting at the bottom with a [cluster](https://docs.ray.io/en/latest/cluster/getting-started.html), Ray sets up and manages clusters of computers so that you can run distributed applications on them. You can deploy a Ray cluster on AWS, GCP or on Kubernetes via the officially supported [KubeRay](https://docs.ray.io/en/latest/cluster/kubernetes/index.html) project. \n",
    "\n",
    "Note: [Anyscale](https://www.anyscale.com/), the company behind Ray, builds enterprise-ready AI compute platform for running and managing Ray applications.\n",
    "\n",
    "#### Ray Core\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/walkthrough.html\" target=\"_blank\">Ray Core</a></strong> is an open-source, Python, general purpose, distributed computing library that enables ML engineers and Python developers to scale Python applications and accelerate machine learning workloads.\n",
    "</div>\n",
    "\n",
    "Ray Core is the foundation that Ray's ML libraries (Ray AIR) and third-party integrations (Ray ecosystem) are built on. This library enables Python developers to easily build scalable, distributed systems that can run on a laptop, cluster, cloud or Kubernetes.\n",
    "\n",
    "Expanding on the key abstractions mentioned before:\n",
    "\n",
    "1. [**Tasks**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks). Remote, stateless Python functions.  \n",
    "Ray tasks are arbitrary Python functions that are executed asychronously on separate Python workers on a Ray cluster nodes. Users can specify their resource requirements in terms of CPUs, GPUs, and custom resources which are used by the cluster scheduler to distribute tasks for parallelized execution.\n",
    "\n",
    "2. [**Actors**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors). Remote stateful Python classes.  \n",
    "What tasks are to functions, actors are to classes. An actor is a stateful worker, and the methods of an actor are scheduled on that specific worker and can access and mutate the state of that worker. Like tasks, actors support CPU, GPU, and custom resource requirements.\n",
    "\n",
    "3. [**Objects**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects). In-memory, immutable objects or values that can be accessed anywhere in the computing cluster.  \n",
    "In Ray, tasks and actors create and compute on objects. These remote objects can be stored anywhere in a Ray cluster. Object References are used to refer to them, and they are cached in Ray's distributed shared memory object store.\n",
    "\n",
    "#### Ray AI Runtime\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python, domain-specific set of libraries that equip ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "Ray AIR is built on top of Ray Core and focuses on distributed both individual and end-to-end machine learning workflows.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Introduction_to_Ray_AIR/e2e_air.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Ray AIR enables end-to-end ML development and provides multiple options to integrate with other tools and libraries from the MLOps ecosystem.|\n",
    "\n",
    "Each of the five native libraries that Ray AIR wraps distributes a specific ML task:\n",
    "\n",
    "1. [**Ray Data**](https://docs.ray.io/en/latest/data/dataset.html)  \n",
    "Scalable, framework-agnostic data loading and transformation across training, tuning, and prediction.\n",
    "    \n",
    "2. [**Ray Train**](https://docs.ray.io/en/latest/train/train.html)  \n",
    "Distributed multi-node and multi-core model training with fault tolerance that integrates with popular training libraries.\n",
    "\n",
    "3. [**Ray Tune**](https://docs.ray.io/en/latest/tune/index.html)  \n",
    "Scales hyperparameter tuning to optimize model performance.\n",
    "\n",
    "4. [**Ray Serve**](https://docs.ray.io/en/latest/serve/index.html)  \n",
    "Deploys models for online inference, with optional microbatching to improve performance.\n",
    "\n",
    "5. [**Ray RLlib**](https://docs.ray.io/en/latest/rllib/index.html)  \n",
    "Distributed reinforcement learning workloads that integrate with the other Ray AIR libraries.\n",
    "\n",
    "#### Integrations and ecosystem libraries\n",
    "\n",
    "Ray integrates with a [growing ecosystem](https://docs.ray.io/en/latest/ray-overview/ray-libraries.html) of the most popular Python and machine learning libraries and frameworks. Instead of trying to impose a new standards, Ray allows you to scale existing workloads by unifying tools in a common interface. This interface enables you to run ML tasks in a distributed way, a property most of the respective backends don't have, or not to the same extent.\n",
    "\n",
    "Here are a handful of integrations to highlight:\n",
    "\n",
    "* **Ray Datasets**\n",
    "  * [Dask](https://docs.ray.io/en/latest/ray-more-libs/dask-on-ray.html)\n",
    "  * [Pandas/Modin](https://docs.ray.io/en/latest/data/modin/index.html)\n",
    "* **Ray Train and RLlib** \n",
    "  * [Tensorflow](https://docs.ray.io/en/latest/train/api.html#ray.train.tensorflow.TensorflowTrainer)\n",
    "  * [PyTorch](https://docs.ray.io/en/latest/train/api.html#ray.train.torch.TorchTrainer)\n",
    "* **Ray Tune**\n",
    "  * [HyperOpt](https://docs.ray.io/en/latest/tune/examples/hyperopt_example.html)\n",
    "  * [Optuna](https://docs.ray.io/en/latest/tune/examples/optuna_example.html)\n",
    "* **Ray Serve**\n",
    "  * [FastAPI](https://www.anyscale.com/blog/ray-serve-fastapi-the-best-of-both-worlds)\n",
    "  * [gradio](https://docs.ray.io/en/latest/serve/tutorials/gradio-integration.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Ray use cases\n",
    "\n",
    "Now that you have a sense of what Ray is in theory, it's important to discuss what makes Ray so useful in practice. In this section, you will encounter the ways that individuals, organizations, and companies leverage Ray to build their AI applications.\n",
    "\n",
    "First, you will explore how Ray scales common ML workloads. Then, you will about some advanced implementations.\n",
    "\n",
    "#### Scaling common ML workloads\n",
    "\n",
    "##### Parallel training of many models\n",
    "When any given model you want to train can fit on a single GPU, then Ray can assign each training run to a separate Ray Task. In this way, all available workers are utilized to run independent remote training rather than one worker running jobs sequentially.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/training_small_models.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Data parallelism pattern for distributed training on large datasets.|\n",
    "\n",
    "##### Distributed training of large models\n",
    "In contrast to training many models, model parallelism partitions a large model across many machines for training. Ray Train has built-in abstractions for distributing shards of models and running training in parallel.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/model_parallelism.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Model parallelism pattern for distributed large model training.|\n",
    "\n",
    "##### Managing parallel hyperparameter tuning experiments\n",
    "Running multiple hyperparameter tuning experiments is a pattern apt for distributed computing because each experiment is independent of one another. Ray Tune handles the hard bit of distributing hyperparameter optimization and makes available key features such as checkpointing the best result, optimizing scheduling, specifying search patterns, and more.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/tuning_use_case.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Distributed tuning with distributed training per trial.|\n",
    "\n",
    "##### Reinforcement learning\n",
    "Ray RLlib offers support for production-level, distributed reinforcement learning workloads while maintaining unified and simple APIs for a large variety of industry applications.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/rllib_use_case.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Decentralized distributed proximal polixy optimiation (DD-PPO) architecture, supported by Ray RLLib, where sampling and training are done on worker GPUs.|\n",
    "\n",
    "##### Batch inference on CPUs and GPUs\n",
    "Performing inference on incoming batches of data can be parallelized by exporting the architecture and weights of a trained model to the shared object store. Then, using these model replicas, Ray scales predictions on batches across workers.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/batch_inference.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Using Ray AIR's `BatchPredictor` for batch inference.|\n",
    "\n",
    "##### Multi-model composition for model serving\n",
    "\n",
    "[Ray Serve](https://docs.ray.io/en/latest/serve/index.html) supports complex [model deployment patterns](https://www.youtube.com/watch?v=mM4hJLelzSw) requiring the orchestration of multiple Ray actors, where different actors provide inference for different models. Serve handles both batch and online inference and can scale to thousands of models in production.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/multi_model_serve.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Deployment patterns with Ray Serve.|\n",
    "\n",
    "##### ML platform\n",
    "\n",
    "[Merlin](https://shopify.engineering/merlin-shopify-machine-learning-platform) is Shopify's ML platform built on Ray. It enables fast-iteration and [scaling of distributed applications](https://www.youtube.com/watch?v=kbvzvdKH7bc) such as product categorization and recommendations.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/shopify.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Merlin architecture built on Ray.|\n",
    "\n",
    "Spotify [uses Ray for advanced applications](https://www.anyscale.com/ray-summit-2022/agenda/sessions/180) that include personalizing content recommendations for home podcasts and personalizing Spotify Radio track sequencing.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/spotify.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|How Ray ecosystem empowers ML scientists and engineers at Spotify.|\n",
    "\n",
    "#### Implementing advanced ML workloads\n",
    "\n",
    "##### Alpa - training very large models with Ray\n",
    "\n",
    "[Alpa](https://ai.googleblog.com/2022/05/alpa-automated-model-parallel-deep.html) is a Ray-native library that automatically partitions, schedules, and executes the training and serving computation of [very large deep learning models](https://www.anyscale.com/ray-summit-2022/agenda/sessions/170) on hundreds of GPUs.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/alpa.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Parallelization plans for a computational graph from Alpa. A, B, C, and D are operators. Each color represents a different device (i.e. GPU) executing a partition or the full operator leveraging Ray actors.|\n",
    "\n",
    "##### Exoshuffle - large scale data shuffling\n",
    "\n",
    "In Ray 2.0, [Exoshuffle](https://cs.paperswithcode.com/paper/exoshuffle-large-scale-shuffle-at-the) is integrated with the Ray Data to provide an application level shuffle system that [outperforms Spark](https://www.anyscale.com/ray-summit-2022/agenda/sessions/220) and achieves 82% of theoretical performance on a 100TB sort on 100 nodes.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/exoshuffle.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Shuffle on Ray architecture diagram.|\n",
    "\n",
    "##### Hamilton - feature engineering\n",
    "\n",
    "[Hamilton](https://github.com/stitchfix/hamilton) is an open source dataflow micro-framework that manages feature engineering for time series forecasting. Developed at [StitchFix](https://www.anyscale.com/ray-summit-2022/agenda/sessions/115), this library provides scalable parallelism, where each Hamilton function is distributed and data is limited by machine memory.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/stitchfix.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Hamilton architecture on Ray clusters.|\n",
    "\n",
    "##### Riot Games - reinforcement learning\n",
    "\n",
    "Riot Games uses [Ray to build bots](https://www.anyscale.com/ray-summit-2022/agenda/sessions/148) that play new battles at various skill levels. These reinforcement learning agents provide additional signals to their designers to deliver the best experiences for players.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/riot.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Riot reinforcement learning workflow on Ray including data transformation and training.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Part 1\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/map_layers_only.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Stack of Ray libraries - unified toolkit for ML workloads.|\n",
    "\n",
    "To review, this section has covered the following topics:\n",
    "\n",
    "* Introduction to the Ray project\n",
    "* Key characteristics of Ray\n",
    "* The three layers of Ray\n",
    "    * Ray Core\n",
    "    * Native libraries\n",
    "    * Ecosystem of integrations\n",
    "* Common Ray use cases\n",
    "\n",
    "#### Concepts\n",
    "\n",
    "Key concepts introduced in Part 1:\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://www.ray.io/\" target=\"_blank\">Ray</a></strong> is an open-source unified compute framework that makes it easy to scale AI and Python workloads.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/cluster/getting-started.html\" target=\"_blank\">Ray clusters</a></strong> are a set of worker nodes connected to a common Ray head node. Ray clusters can be fixed-size, or they can autoscale up and down according to the resources requested by applications running on the cluster.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-core/walkthrough.html\" target=\"_blank\">Ray Core</a></strong> is an open-source, Python, general purpose, distributed computing library that enables ML engineers and Python developers to accelerate machine learning workloads and scale Python applications.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "  <strong><a href=\"https://docs.ray.io/en/latest/ray-air/getting-started.html\" target=\"_blank\">Ray AI Runtime (AIR)</a></strong> is an open-source, Python, domain specific library that equips ML engineers, data scientists, and researchers with a scalable and unified toolkit for ML applications.\n",
    "</div>\n",
    "\n",
    "#### APIs and technical abstractions\n",
    "\n",
    "Three Ray Core abstractions that enable parallel computation:\n",
    "\n",
    "1. [**Tasks**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks). Remote, stateless Python functions\n",
    "1. [**Actors**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors). Remote, stateful Python classes\n",
    "1. [**Objects**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects). Tasks and actors create and compute on objects that can be stored and accessed anywhere in the cluster; cached in Ray's distributed [shared-memory](https://en.wikipedia.org/wiki/Shared_memory) object store\n",
    "\n",
    "#### What's next?\n",
    "\n",
    "Apply Ray concepts by running an illustrative code example that will demonstrate how to distribute a simple application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Hands-on code example - scaling regression with Ray Core"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "To gain a better feel for Ray, this section will scale a bare bones version of a common ML task: regression on structured data.\n",
    "\n",
    "#### Data\n",
    "\n",
    "You will be performing regression on the [California House Prices](https://scikit-learn.org/stable/datasets/real_world.html#california-housing-dataset) dataset made available by scikit-learn.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/California_dataset.png\" width=\"80%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|`n_samples = 20640`, target is numeric and corresponds to the average house value in units of 100k.|\n",
    "\n",
    "#### Model and task\n",
    "\n",
    "You will train and score [random forest](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html) models using [mean squared error](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html) as the metric.\n",
    "\n",
    "In a lightweight version of hyperparameter tuning, you will be training many models with varying values of `n_estimators`. First, you will encounter a sequential version of model training where each experiment executes in series one after another. Then, you will distribute these training runs with Ray Core to achieve better performance and faster model training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential implementation\n",
    "\n",
    "Starting with a familiar implementation, an assortment of random forest models are trained one by one sequentially as depicted in the diagram below.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/sequential_timeline.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Timeline of sequential tasks all on one worker. Each \"task\" in this case is training a random forest model.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from operator import itemgetter\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.datasets import fetch_kddcup99\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_raw, y_raw = fetch_california_housing(return_X_y=True, as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  \\\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88   \n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86   \n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85   \n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85   \n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85   \n",
       "\n",
       "   Longitude  \n",
       "0    -122.23  \n",
       "1    -122.22  \n",
       "2    -122.24  \n",
       "3    -122.25  \n",
       "4    -122.25  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_raw.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "X = copy.deepcopy(X_raw)\n",
    "y = copy.deepcopy(y_raw)\n",
    "for i in range(50):\n",
    "    X = pd.concat([X, X_raw])\n",
    "    y = pd.concat([y, y_raw])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=201\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set number of models to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You will use NUM_MODELS as a benchmark to compare performance\n",
    "# across sequential and parallel implementations.\n",
    "\n",
    "NUM_MODELS = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement function to train and score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_score_model(\n",
    "    train_set: pd.DataFrame,\n",
    "    test_set: pd.DataFrame,\n",
    "    train_labels: pd.Series,\n",
    "    test_labels: pd.Series,\n",
    "    n_estimators: int,\n",
    "):\n",
    "    start_time = time.time()  # measure wall time for single model training\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=201)\n",
    "    model.fit(train_set, train_labels)\n",
    "    y_pred = model.predict(test_set)\n",
    "    score = mean_squared_error(test_labels, y_pred)\n",
    "\n",
    "    time_delta = time.time() - start_time\n",
    "    print(\n",
    "        f\"n_estimators={n_estimators}, mse={score:.4f}, took: {time_delta:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    return n_estimators, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes data, creates a `RandomForestRegressor` model, trains it and scores the model on the test set.\n",
    "\n",
    "`train_and_score_model` returns a tuple:\n",
    "```\n",
    "(n_estimators, mse_score)\n",
    "```\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "(8, 0.2983)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement function that runs **sequential** model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sequential(n_models: int):\n",
    "    return [\n",
    "        train_and_score_model(\n",
    "            train_set=X_train,\n",
    "            test_set=X_test,\n",
    "            train_labels=y_train,\n",
    "            test_labels=y_test,\n",
    "            n_estimators=8 + 4 * j,\n",
    "        )\n",
    "        for j in range(n_models)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function trains `n_models` sequentially for an increasing number of `n_estimators` (increasing by 4 for each model, e.g. 8, 12, 16, 20, ...). \n",
    "\n",
    "`run_sequential` returns a list of tuples:\n",
    "```\n",
    "[(n_estimators, mse_score), (n_estimators, mse_score), ...]\n",
    "```\n",
    "\n",
    "For example:\n",
    "\n",
    "```\n",
    "[(8, 0.2983), (12, 0.2826), (16, 0.2761), (24, 0.2694)]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run sequential model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators=8, mse=0.0000, took: 5.40 seconds\n",
      "n_estimators=12, mse=0.0000, took: 8.11 seconds\n",
      "n_estimators=16, mse=0.0000, took: 10.81 seconds\n",
      "n_estimators=20, mse=0.0000, took: 13.52 seconds\n",
      "n_estimators=24, mse=0.0000, took: 16.22 seconds\n",
      "n_estimators=28, mse=0.0000, took: 18.94 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:1\u001b[0m\n",
      "Cell \u001b[0;32mIn [8], line 2\u001b[0m, in \u001b[0;36mrun_sequential\u001b[0;34m(n_models)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_sequential\u001b[39m(n_models: \u001b[38;5;28mint\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m      3\u001b[0m         train_and_score_model(\n\u001b[1;32m      4\u001b[0m             train_set\u001b[38;5;241m=\u001b[39mX_train,\n\u001b[1;32m      5\u001b[0m             test_set\u001b[38;5;241m=\u001b[39mX_test,\n\u001b[1;32m      6\u001b[0m             train_labels\u001b[38;5;241m=\u001b[39my_train,\n\u001b[1;32m      7\u001b[0m             test_labels\u001b[38;5;241m=\u001b[39my_test,\n\u001b[1;32m      8\u001b[0m             n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;241m*\u001b[39m j,\n\u001b[1;32m      9\u001b[0m         )\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_models)\n\u001b[1;32m     11\u001b[0m     ]\n",
      "Cell \u001b[0;32mIn [8], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_sequential\u001b[39m(n_models: \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m----> 3\u001b[0m         \u001b[43mtrain_and_score_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtest_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_estimators\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_models)\n\u001b[1;32m     11\u001b[0m     ]\n",
      "Cell \u001b[0;32mIn [7], line 11\u001b[0m, in \u001b[0;36mtrain_and_score_model\u001b[0;34m(train_set, test_set, train_labels, test_labels, n_estimators)\u001b[0m\n\u001b[1;32m      8\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# measure wall time for single model training\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39mn_estimators, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m201\u001b[39m)\n\u001b[0;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(test_set)\n\u001b[1;32m     13\u001b[0m score \u001b[38;5;241m=\u001b[39m mean_squared_error(test_labels, y_pred)\n",
      "File \u001b[0;32m~/kishu-venv/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:476\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    465\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    466\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    468\u001b[0m ]\n\u001b[1;32m    470\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 476\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/kishu-venv/lib/python3.8/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/kishu-venv/lib/python3.8/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/kishu-venv/lib/python3.8/site-packages/sklearn/utils/fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[0;32m--> 117\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kishu-venv/lib/python3.8/site-packages/sklearn/ensemble/_forest.py:189\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    187\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 189\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m     tree\u001b[38;5;241m.\u001b[39mfit(X, y, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/kishu-venv/lib/python3.8/site-packages/sklearn/tree/_classes.py:1342\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m   1314\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[1;32m   1315\u001b[0m \n\u001b[1;32m   1316\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1339\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[1;32m   1340\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1342\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1344\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1345\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/kishu-venv/lib/python3.8/site-packages/sklearn/tree/_classes.py:458\u001b[0m, in \u001b[0;36mBaseDecisionTree.fit\u001b[0;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    449\u001b[0m         splitter,\n\u001b[1;32m    450\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    456\u001b[0m     )\n\u001b[0;32m--> 458\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "mse_scores = run_sequential(n_models=NUM_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: wall time on an M1 MacBook Pro: 1min (60s)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mse_scores' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(\u001b[43mmse_scores\u001b[49m, key\u001b[38;5;241m=\u001b[39mitemgetter(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest model: mse=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, n_estimators=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'mse_scores' is not defined"
     ]
    }
   ],
   "source": [
    "best = min(mse_scores, key=itemgetter(1))\n",
    "print(f\"Best model: mse={best[1]:.4f}, n_estimators={best[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results of training, make a note on how long training `NUM_MODELS` sequentially took. Continue on to the next section to learn about how to improve runtime by distributing this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallel implementation\n",
    "\n",
    "In contrast to the previous approach, you will now utilize all available resources to train these models in parallel. Ray will automatically detect the number of cores on your computer or the amount of resources in a cluster to distribute each defined task.\n",
    "\n",
    "The diagram below offers an intuition for how tasks are assigned and executed in a parallel approach. You will notice that this introduces a scheduler which is responsible for managing incoming requests, assigning nodes, and detecting available resources.\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/distributed_timeline.png\" width=\"80%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|A generic timeline with ten tasks running across 4 workers in parallel, with minor overhead from the scheduler.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "\n",
    "if ray.is_initialized:\n",
    "    ray.shutdown()\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Begin by running `ray.init()` to start a fresh Ray cluster and take a look at some useful information:\n",
    "\n",
    "* Python version\n",
    "* Ray version\n",
    "* Link to Ray Dashboard: an observability tool that provides insight into what Ray is doing via helpful metrics and charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put data in the object store\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/object_store.png\" width=\"70%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Workers use `ray.put()` to place objects and use `ray.get()` to retrieve them from each node's object store. These object stores form the shared distributed memory that makes objects available across a Ray cluster.|\n",
    "\n",
    "In a distributed system, object references are pointers to objects in memory. Object references can be used to access objects that are stored on different machines, allowing them to communicate with each other and share data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ref = ray.put(X_train)\n",
    "X_test_ref = ray.put(X_test)\n",
    "y_train_ref = ray.put(y_train)\n",
    "y_test_ref = ray.put(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By placing the training and testing data into Ray's object store, these objects are now available to all remote tasks and actors in the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Coding Exercise**\n",
    "\n",
    "To practice working with object references, use the cell below to:\n",
    "\n",
    "1. Print what `X_train_ref` looks like.\n",
    "2. Retrieve `X_train` by using `ray.get()` on the object reference.\n",
    "\n",
    "An example Object Reference looks like this:\n",
    "\n",
    "`ObjectRef(00ffffffffffffffffffffffffffffffffffffff0100000002000000)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### SAMPLE IMPLEMENTATION ###\n",
    "\n",
    "# print the object reference\n",
    "print(X_train_ref)\n",
    "\n",
    "# inspect the in-memory object\n",
    "ray.get(X_train_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement function to train and score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@ray.remote\n",
    "def train_and_score_model(\n",
    "    train_set_ref: pd.DataFrame,\n",
    "    test_set_ref: pd.DataFrame,\n",
    "    train_labels_ref: pd.Series,\n",
    "    test_labels_ref: pd.Series,\n",
    "    n_estimators: int,\n",
    "):\n",
    "    start_time = time.time()  # measure wall time for single model training\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=n_estimators, random_state=201)\n",
    "    model.fit(train_set_ref, train_labels_ref)\n",
    "    y_pred = model.predict(test_set_ref)\n",
    "    score = mean_squared_error(test_labels_ref, y_pred)\n",
    "\n",
    "    time_delta = time.time() - start_time\n",
    "    print(\n",
    "        f\"n_estimators={n_estimators}, mse={score:.4f}, took: {time_delta:.2f} seconds\"\n",
    "    )\n",
    "\n",
    "    return n_estimators, score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `train_and_score_model` is *the same function* as in the sequential example, except here, you add the `@ray.remote` decorator to specify that this function will be executed in a distributed manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement function that runs **parallel** model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_parallel(n_models: int):\n",
    "    results_ref = [\n",
    "        train_and_score_model.remote(\n",
    "            train_set_ref=X_train_ref,\n",
    "            test_set_ref=X_test_ref,\n",
    "            train_labels_ref=y_train_ref,\n",
    "            test_labels_ref=y_test_ref,\n",
    "            n_estimators=8 + 4 * j,\n",
    "        )\n",
    "        for j in range(n_models)\n",
    "    ]\n",
    "    return ray.get(results_ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before, you defined `run_sequential()` to train and score `NUM_MODELS`. Working from the inside-out, modifying this into `run_parallel()` involves three steps:\n",
    "\n",
    "1. Append a `.remote` postfix to `train_and_score_model`.  \n",
    "    * Remember that you specified this function as a remote task in the previous cell. In Ray, you append this suffix to every remote call.\n",
    "2. Capture the resulting list of object references in `results_ref`.\n",
    "    * Rather than waiting for the results, you immediately receive a list of references to results that are expected to be available in the future. This asychronous (non-blocking) call allows a program to continue executing other operations while the potentially time-consuming operations can be computed in the background.\n",
    "3. Access results with `ray.get()`.\n",
    "    * Once all models have been assigned to workers, call `ray.get()` on the list of object references `results_ref` to retrieve completed results. This is a synchronous (blocking) operation because it waits until all computation on objects complete.\n",
    "\n",
    "For example,\n",
    "\n",
    "```\n",
    "ray.get([ObjectRef, ObjectRef, ObjectRef, ...])\n",
    "```\n",
    "\n",
    "returns list of `(n_estimators, score)` tuples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run parallel model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mse_scores = run_parallel(n_models=NUM_MODELS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice **6x performance gain**:\n",
    "\n",
    "* Parallel: 10s\n",
    "* Sequential: 1min (60s)\n",
    "\n",
    "\n",
    "*(experiment on the M1 MacBook Pro)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = min(mse_scores, key=itemgetter(1))\n",
    "print(f\"Best model: mse={best[1]:.4f}, n_estimators={best[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training has completed with a **6x performance gain** due to parallel execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shutdown Ray runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.shutdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disconnect the worker and terminate processes started by `ray.init()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of Part 2: code example\n",
    "\n",
    "You achieved a significant performance gain by introducing parallel model training. You adapted a sequential model training computational job to run in parallel by using the Ray Core API.\n",
    "\n",
    "With Ray, you parallelized training without having to implement the orchestration, fault tolerance or autoscaling component that requires specialized knowledge of distributed systems.\n",
    "\n",
    "#### Key concepts\n",
    "\n",
    "1. [**Tasks**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#tasks). Remote, stateless Python functions\n",
    "1. [**Actors**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#actors). Remote, stateful Python classes\n",
    "1. [**Objects**](https://docs.ray.io/en/latest/ray-core/key-concepts.html#objects). Tasks and actors create and compute on objects that can be stored and accessed anywhere in the cluster; cached in Ray's distributed [shared-memory](https://en.wikipedia.org/wiki/Shared_memory) object store\n",
    "\n",
    "#### Key API elements\n",
    "\n",
    "* **`ray.init()`**  \n",
    "Start Ray runtime and connect to the Ray cluster.\n",
    "* **`@ray.remote`**  \n",
    "Decorator that specifies a Python function or class to be executed as a task (remote function) or actor (remote class) in a different process.\n",
    "* **`.remote`**  \n",
    "Postfix to the remote functions and classes; remote operations are *asynchronous*.\n",
    "* **`ray.put()`**  \n",
    "Put an object in the in-memory object store; returns an object reference used to pass the object to any remote function or method call.\n",
    "* **`ray.get()`**  \n",
    "Get a remote object(s) from the object store by specifying the object reference(s).\n",
    "\n",
    "|<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Overview_of_Ray/side_by_side.png\" width=\"100%\" loading=\"lazy\">|\n",
    "|:--|\n",
    "|Comparison of sample workflow with minimal code changes needed to distribute tasks on Ray.|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect with the Ray community\n",
    "\n",
    "You can learn and get more involved with the Ray community of developers and researchers:\n",
    "\n",
    "* [**Ray documentation**](https://docs.ray.io/en/latest)\n",
    "\n",
    "* [**Official Ray site**](https://www.ray.io/)  \n",
    "Browse the ecosystem and use this site as a hub to get the information that you need to get going and building with Ray.\n",
    "\n",
    "* [**Join the community on Slack**](https://forms.gle/9TSdDYUgxYs8SA9e8)  \n",
    "Find friends to discuss your new learnings in our Slack space.\n",
    "\n",
    "* [**Use the discussion board**](https://discuss.ray.io/)  \n",
    "Ask questions, follow topics, and view announcements on this community forum.\n",
    "\n",
    "* [**Join a meetup group**](https://www.meetup.com/Bay-Area-Ray-Meetup/)  \n",
    "Tune in on meet-ups to listen to compelling talks, get to know other users, and meet the team behind Ray.\n",
    "\n",
    "* [**Open an issue**](https://github.com/ray-project/ray/issues/new/choose)  \n",
    "Ray is constantly evolving to improve developer experience. Submit feature requests, bug-reports, and get help via GitHub issues.\n",
    "\n",
    "* [**Become a Ray contributor**](https://docs.ray.io/en/latest/ray-contribute/getting-involved.html)  \n",
    "We welcome community contributions to improve our documentation and Ray framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://technical-training-assets.s3.us-west-2.amazonaws.com/Generic/ray_logo.png\" width=\"20%\" loading=\"lazy\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "567405a8058597909526349386224fe35dd047505a91307e44ed44be00113429"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
